# config/llm/local.yaml
# Local LLM configuration (e.g., Ollama, LM Studio)

provider: local
model: llama3.1
temperature: 0.0
max_tokens: 2048
timeout: 120
max_retries: 2

local:
  api_base: http://localhost:11434
  model_path: null
  
available_models:
  - llama3.1
  - mistral
  - codellama
